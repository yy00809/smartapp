{"nbformat_minor": 0, "cells": [{"source": "## Spark ML using logistic Regression  \nThe problem here is to predict the grade (Passing or Failing) of City of Chicago restaurant inspections based on the notes made by the inspector.  \n\nThis uses logistic regression.  Suppose you have a set of feature vectors $x_i \\in R^n$ for $i$ in in $[0,m]$.  Associated with each feature vector we have a binary result $y_i$.  We are interested in the probability $P(y =1 | x)$ which we write as the function $p(x)$. However because $p(x)$ is between 0 and 1 it is not expressable as a linear function of x so we can't use regular linear regression, so we look at the odds expression $p(x) / (1-p(x))$ and make the guess that its log is linear. In other words\n\n$$ ln( \\frac{p(x)}{1-p(x)}) = b_0 + b \\cdot x$$\n\nwhere the offset $b_0$ and the vector $b = [b_1, b_2, ... b_n]$ define a hyperplane for linear regression.  solving this for $p(x)$ we get\n\n$$p(x) = \\frac {1}{1+e^{-(b_0 + b \\cdot x)}}  $$\n\nAnd we say $y=1$  if $p(x)>0$  otherwise it is zero.   Unfortunately finding the best $b_0$ and $b$ is not as easy as straight linear regression, but simple Newton like iterations will converge to good solutions. \n\n\nWe note that the logistic function $\\sigma (t)$ is defined as follows:\n\n$$\\sigma (t)= \\frac {e^t}{e^{t}+1} =\\frac {1}{1+e^{-t}}$$\n\nIt is used frequently in machine learning to map a real number into a probabilty range $[0,1]$ .\n\n\n ", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "s", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 41.06103515625, "end_time": 1626864570906.799}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 1, "cell_type": "code", "source": "\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import *\nfrom pyspark import SparkContext\n\nspark = SparkContext.getOrCreate()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1626860397864_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-eeee11.4ae3eejnjdyuvmxg4twhrtnhje.px.internal.cloudapp.net:8088/proxy/application_1626860397864_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-eeee11.4ae3eejnjdyuvmxg4twhrtnhje.px.internal.cloudapp.net:30060/node/containerlogs/container_1626860397864_0010_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 250.900146484375, "end_time": 1626868989246.516}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": "from pyspark import SQLContext\n\nsqlCtx = SQLContext(spark)\n\n\ndef csvParse(s):\n    import csv\n    from StringIO import StringIO\n    sio = StringIO(s)\n    value = csv.reader(sio).next()\n    sio.close()\n    return value", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 39.97607421875, "end_time": 1626869001209.257}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## The version in this notebook uses a slightly different input file from the one in the Azure HDInsight demo.   \nThis notebook will run on spark on your laptop.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": "inspections = spark.textFile('wasb:///HdiSamples/HdiSamples/FoodInspectionData/Food_Inspections1.csv')\\\n                .map(csvParse)\n\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 454.843994140625, "end_time": 1626869103952.042}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 6, "cell_type": "code", "source": "inspections.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "13101"}], "metadata": {"cell_status": {"execute_time": {"duration": 5309.7509765625, "end_time": 1626869110933.511}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 7, "cell_type": "code", "source": "inspections.take(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[['413707', 'LUNA PARK INC', 'LUNA PARK  DAY CARE', '2049789', \"Children's Services Facility\", 'Risk 1 (High)', '3250 W FOSTER AVE ', 'CHICAGO', 'IL', '60625', '09/21/2010', 'License-Task Force', 'Fail', '24. DISH WASHING FACILITIES: PROPERLY DESIGNED, CONSTRUCTED, MAINTAINED, INSTALLED, LOCATED AND OPERATED - Comments: All dishwashing machines must be of a type that complies with all requirements of the plumbing section of the Municipal Code of Chicago and Rules and Regulation of the Board of Health. OBSEVERD THE 3 COMPARTMENT SINK BACKING UP INTO THE 1ST AND 2ND COMPARTMENT WITH CLEAR WATER AND SLOWLY DRAINING OUT. INST NEED HAVE IT REPAIR. CITATION ISSUED, SERIOUS VIOLATION 7-38-030 H000062369-10 COURT DATE 10-28-10 TIME 1 P.M. ROOM 107 400 W. SURPERIOR. | 36. LIGHTING: REQUIRED MINIMUM FOOT-CANDLES OF LIGHT PROVIDED, FIXTURES SHIELDED - Comments: Shielding to protect against broken glass falling into food shall be provided for all artificial lighting sources in preparation, service, and display facilities. LIGHT SHIELD ARE MISSING UNDER HOOD OF  COOKING EQUIPMENT AND NEED TO REPLACE LIGHT UNDER UNIT. 4 LIGHTS ARE OUT IN THE REAR CHILDREN AREA,IN THE KINDERGARDEN CLASS ROOM. 2 LIGHT ARE OUT EAST REAR, LIGHT FRONT WEST ROOM. NEED TO REPLACE ALL LIGHT THAT ARE NOT WORKING. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: The walls and ceilings shall be in good repair and easily cleaned. MISSING CEILING TILES WITH STAINS IN WEST,EAST, IN FRONT AREA WEST, AND BY THE 15MOS AREA. NEED TO BE REPLACED. | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: All food and non-food contact equipment and utensils shall be smooth, easily cleanable, and durable, and shall be in good repair. SPLASH GUARDED ARE NEEDED BY THE EXPOSED HAND SINK IN THE KITCHEN AREA | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: The floors shall be constructed per code, be smooth and easily cleaned, and be kept clean and in good repair. INST NEED TO ELEVATE ALL FOOD ITEMS 6INCH OFF THE FLOOR 6 INCH AWAY FORM WALL.  ', '41.97583445690982', '-87.7107455232781', '(41.97583445690982, -87.7107455232781)'], ['391234', 'CAFE SELMARIE', 'CAFE SELMARIE', '1069067', 'Restaurant', 'Risk 1 (High)', '4729 N LINCOLN AVE ', 'CHICAGO', 'IL', '60625', '09/21/2010', 'Canvass', 'Fail', \"2. FACILITIES TO MAINTAIN PROPER TEMPERATURE - Comments: All food establishments that display, prepare, or store potentially hazardous food shall have adequate refrigerated food storage facilities. All food establishments that prepare, sell, or store hot food shall have adequate hot food storage facilities.//MGMT & I OBSERVED REACH IN COOLER IN KITCHEN IN  FRONT OF GRILL TEMP 45.0F PLACE HELD FOR INSPECTION TAG ON UNIT ,NOT TO USE UNTIL REINSPECTED BY DEPT OF HEALTH | 3. POTENTIALLY HAZARDOUS FOOD MEETS TEMPERATURE REQUIREMENT DURING STORAGE, PREPARATION DISPLAY AND SERVICE - Comments: All cold food shall be stored at a temperature of 40F or less.//MGMT & I OBSERVED POTENTIALLY HAZARDOUS FOODS AT IMPROPER TEMPS(RAW SALMON,SOUR CREAM,GUACAMOLE,PENNE PASTA CHICKEN BREAST,CRAB CAKES,WHOLE QUICHI, TEMPS BETWEEN 45.0F-46.5F,MGMT VOLUNTEERED TO PROPERLY DISCARED FOOD ITEMS $300.00 | 19. OUTSIDE GARBAGE WASTE GREASE AND STORAGE AREA; CLEAN, RODENT PROOF, ALL CONTAINERS COVERED - Comments: The area outside of the establishment used for the storage of garbage shall be clean at all times and shall not constitute a nuisance.//OUT SIDE GARBAGE FOR USED CARD BOARD BOXES NO LIDE OR COVER FOR CONTANER NEED COVER & PICK UP. | 30. FOOD IN ORIGINAL CONTAINER, PROPERLY LABELED: CUSTOMER ADVISORY POSTED AS NEEDED - Comments: All food not stored in the original container shall be stored in properly labeled containers.NEED NAMES ,DATES ON ALL FOOD ITEMS ON DISPLAY ,IN STORAGE,INGREDENTS ,NAME OF FOOD ITEM,EXP. DATE,NAME OF CO. ON PREPACKEGE FOOD ITEMS ON DISPLAY. | 31. CLEAN MULTI-USE UTENSILS AND SINGLE SERVICE ARTICLES PROPERLY STORED: NO REUSE OF SINGLE SERVICE ARTICLES - Comments: Containers and utensils shall be inverted, covered, or otherwise protected from contamination until used.  All single-service drinking straws and containers shall be discarded immediately after use.//INVERT ALL POTS PANS IN STORAGE. | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: All food and non-food contact equipment and utensils shall be smooth, easily cleanable, and durable, and shall be in good repair.//INSTALL SPLASH GUARD BETWEEN EXP. HAND BOWL & FAUCET FOR DRINKING WATER IN REAR AREA,EXP. HAND BOWL & STORAGE SHELVES IN REAR PREP AREA,EXP. HAND BOWL ON ALL SIDES (COUNTER)IN FRONT PREP AREA,REPLACE ALL STAIN & PITTED CUTTING BOARDS IN PREP AREA,NEED LIDS FOR ALL GARBAGE CANS IN PREP AREA WHEN NOT IN USE,NEED PROPER COVERS FOR ALL UNPROTECTED FOOD ITEMS IN STORAGE, | 33. FOOD AND NON-FOOD CONTACT EQUIPMENT UTENSILS CLEAN, FREE OF ABRASIVE DETERGENTS - Comments: All utensils shall be thoroughly cleaned and sanitized after each usage.//SOME SHELVES IN COOLERS DRY STORAGE NEED CLEANING | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: The floors shall be constructed per code, be smooth and easily cleaned, and be kept clean and in good repair.//FLOORS UNDER AROUND STOCK EQUIPMENT NEED CLEANING,MUST STORE ALL STOCK 6' OFF FLOOR WHEN IN STORAGE | 42. APPROPRIATE METHOD OF HANDLING OF FOOD (ICE) HAIR RESTRAINTS AND CLEAN APPAREL WORN - Comments: All employees shall be required to use effective hair restraints to confine hair.//INSTRUCTED MGMT TO HAVE HAIR RESTRAINTS ON PROPERLY IN FOOD PREP AREA\", '41.96740659751604', '-87.68761642361608', '(41.96740659751604, -87.68761642361608)'], ['413751', 'MANCHU WOK', 'MANCHU WOK (T3-H/K FOOD COURT)', '1909522', 'Restaurant', 'Risk 1 (High)', '11601 W TOUHY AVE ', 'CHICAGO', 'IL', '60666', '09/21/2010', 'Canvass', 'Pass', '33. FOOD AND NON-FOOD CONTACT EQUIPMENT UTENSILS CLEAN, FREE OF ABRASIVE DETERGENTS - Comments: All food and non-food contact surfaces of equipment and all food storage utensils shall be thoroughly cleaned and sanitized daily. SHELVES IN WALK IN COOLER MAIN KITCHEN UNCLEAN,MUST CLEAN AND MAINTAIN;AIR VENTS IN WALK IN FREEZER STORAGE AREA (T3 K AND L SIDE)UNCLEAN MUST CLEAN AND MAINTAIN | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: The floors shall be constructed per code, be smooth and easily cleaned, and be kept clean and in good repair. FLOOR IN WALK IN FREEZER IN STORAGE AREA BETWEEN T3K AND L SIDE UNCLEAN,MUST CLEAN AND MAINTAIN | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: All food and non-food contact equipment and utensils shall be smooth, easily cleanable, and durable, and shall be in good repair. EXTERIOR BOTTOM PARTS OF PREP COOLER WHICH IS FACING THE CASHIER UNCLEAN WITH ACCUMULATED DUST MUST CLEAN AND MAINTAIN', '42.008536400868735', '-87.91442843927047', '(42.008536400868735, -87.91442843927047)'], ['413708', 'BENCHMARK HOSPITALITY OF CHICAGO, INC.', 'BENCHMARK HOSPITALITY OF CHICAGO', '2049411', 'Restaurant', 'Risk 1 (High)', '325 N LA SALLE ST BLDG ', 'CHICAGO', 'IL', '60654', '09/21/2010', 'Task Force Liquor 1475', 'Pass', '', '41.88819879207664', '-87.63236298373182', '(41.88819879207664, -87.63236298373182)'], ['413722', 'JJ BURGER', 'JJ BURGER', '2055016', 'Restaurant', 'Risk 2 (Medium)', '749 S CICERO AVE ', 'CHICAGO', 'IL', '60644', '09/21/2010', 'License', 'Pass', '', '41.87082601444883', '-87.74476763884662', '(41.87082601444883, -87.74476763884662)'], ['413752', 'GOLDEN HOOKS FISH & CHICKEN', 'GOLDEN HOOKS FISH & CHICKEN', '2042435', 'Restaurant', 'Risk 2 (Medium)', '3958 W MONROE ST ', 'CHICAGO', 'IL', '60624', '09/21/2010', 'Short Form Complaint', 'Pass', '', '41.87987261425607', '-87.72551692436804', '(41.87987261425607, -87.72551692436804)'], ['413714', 'THE DOCK AT MONTROSE BEACH', 'THE DOCK AT MONTROSE BEACH', '2043260', 'Restaurant', 'Risk 1 (High)', '4400 N SIMONDS DR ', 'CHICAGO', 'IL', '60640', '09/21/2010', 'License', 'Fail', '', '41.96390893734172', '-87.63863624840039', '(41.96390893734172, -87.63863624840039)'], ['413753', 'CLARK FOOD & CIGARETTES INC.,', '', '2042203', 'Grocery Store', 'Risk 3 (Low)', '6761 N CLARK ST BLDG ', 'CHICAGO', 'IL', '60626', '09/21/2010', 'Canvass', 'Pass', '', '42.0053117273606', '-87.67294053846207', '(42.0053117273606, -87.67294053846207)'], ['120580', 'SUSHI PINK', 'SUSHI PINK', '1847340', 'Restaurant', 'Risk 1 (High)', '909 W WASHINGTON BLVD ', 'CHICAGO', 'IL', '60607', '09/21/2010', 'Canvass', 'Pass', '32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: All food and non-food contact equipment and utensils shall be smooth, easily cleanable, and durable, and shall be in good repair.  The cutting boards are rough and unsanitary, resurface or replace the boards so they are smooth and sanitary. | 33. FOOD AND NON-FOOD CONTACT EQUIPMENT UTENSILS CLEAN, FREE OF ABRASIVE DETERGENTS - Comments: All food and non-food contact surfaces of equipment and all food storage utensils shall be thoroughly cleaned and sanitized daily.  Clean the greasy hood, fryer and stove. | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: The floors shall be constructed per code, be smooth and easily cleaned, and be kept clean and in good repair.  Clean the floors under all equipment and tables in the kitchen from corner to corner. | 43. FOOD (ICE) DISPENSING UTENSILS, WASH CLOTHS PROPERLY STORED - Comments: Between uses and during storage ice dispensing utensils and ice receptacles shall be stored in a way that protects them from contamination.', '41.882987317760424', '-87.65014022876997', '(41.882987317760424, -87.65014022876997)'], ['401216', 'M.H.R.,L.L.C.', 'M.H.R.,L.L.C.', '1621323', 'Restaurant', 'Risk 2 (Medium)', '623 S WABASH AVE ', 'CHICAGO', 'IL', '60605', '09/21/2010', 'Canvass', 'Out of Business', '', '41.87390845559158', '-87.62583770570953', '(41.87390845559158, -87.62583770570953)']]"}], "metadata": {"cell_status": {"execute_time": {"duration": 755.223876953125, "end_time": 1626869114268.812}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 8, "cell_type": "code", "source": "schema = StructType([StructField(\"id\", IntegerType(), False), \n                     StructField(\"name\", StringType(), False), \n                     StructField(\"results\", StringType(), False), \n                     StructField(\"violations\", StringType(), True)])\n\ndf = sqlCtx.createDataFrame(inspections.map(lambda l: (int(l[0]), l[2], l[3], l[4])) , schema)\ndf.registerTempTable('CountResults')\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2289.580078125, "end_time": 1626869122350.196}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 9, "cell_type": "code", "source": "df.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------+--------------------+-------+--------------------+\n|    id|                name|results|          violations|\n+------+--------------------+-------+--------------------+\n|413707| LUNA PARK  DAY CARE|2049789|Children's Servic...|\n|391234|       CAFE SELMARIE|1069067|          Restaurant|\n|413751|MANCHU WOK (T3-H/...|1909522|          Restaurant|\n|413708|BENCHMARK HOSPITA...|2049411|          Restaurant|\n|413722|           JJ BURGER|2055016|          Restaurant|\n+------+--------------------+-------+--------------------+\nonly showing top 5 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 2270.843017578125, "end_time": 1626869125606.69}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 10, "cell_type": "code", "source": "print(\"passing = %d\"%df[df.results == 'Pass'].count())\nprint(\"failing = %d\"%df[df.results == 'Fail'].count())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "passing = 0\nfailing = 0"}], "metadata": {"cell_status": {"execute_time": {"duration": 3276.25, "end_time": 1626869130424.705}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 11, "cell_type": "code", "source": "df.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "13101"}], "metadata": {"cell_status": {"execute_time": {"duration": 1255.033935546875, "end_time": 1626869131691.124}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 12, "cell_type": "code", "source": "df.select('results').distinct().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+\n|results|\n+-------+\n|2014004|\n|1801390|\n|1770896|\n|1478647|\n|  20158|\n|1514155|\n|   7252|\n|2032033|\n|1908805|\n|1574657|\n|1800376|\n|  22121|\n|  20428|\n|1597586|\n|  13192|\n|1979986|\n|2037268|\n|2224948|\n|  28117|\n|  35640|\n+-------+\nonly showing top 20 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 2273.992919921875, "end_time": 1626869138224.603}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 13, "cell_type": "code", "source": "#%%sql -o count_results_df\ncount_results_df = sqlCtx.sql(\"SELECT results, COUNT(results) AS cnt FROM \\\n       CountResults GROUP BY results\").toPandas()\nlabels = count_results_df[\"results\"]\nsizes = count_results_df['cnt']\ncolors = ['turquoise', 'seagreen', 'mediumslateblue', 'palegreen', 'coral']\n\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 7293, "end_time": 1626869148357.538}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 15, "cell_type": "code", "source": "count_results_df[\"results\"]", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0       1979986\n1         22121\n2       2037268\n3       2224948\n4         28117\n5         35640\n6         20158\n7         29539\n8       1647924\n9       1042702\n10      2032143\n11      2037497\n12        45141\n13      1675566\n14      2043013\n15        55321\n16      2042268\n17      2021286\n18      2032033\n19        10436\n20      1926867\n21        47940\n22      1801390\n23        64038\n24      2014004\n25      1770896\n26      1478647\n27      1514155\n28         7252\n29      1908805\n         ...   \n8676    1769316\n8677    1620340\n8678    1242839\n8679    2017994\n8680       8507\n8681    2013807\n8682    1981131\n8683    1473964\n8684    2016763\n8685    1422648\n8686    1193152\n8687    1122732\n8688    1908389\n8689      31100\n8690    2224959\n8691       6474\n8692      34229\n8693    2054839\n8694      37566\n8695    1519691\n8696    1927197\n8697    1869952\n8698    2003825\n8699      21665\n8700    1897994\n8701    1968597\n8702    2134640\n8703    1196998\n8704    1880119\n8705    1981894\nName: results, Length: 8706, dtype: object"}], "metadata": {"cell_status": {"execute_time": {"duration": 241.06005859375, "end_time": 1626869165554.986}}, "collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "def labelForResults(s):\n    if s == 'Fail':\n        return 0.0\n    elif s == 'Pass w/ Conditions' or s == 'Pass':\n        return 1.0\n    else:\n        return -1.0\nlabel = UserDefinedFunction(labelForResults, DoubleType())\nlabeledData = df.select(label(df.results).alias('label'), df.violations).where('label >= 0')", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 242.428955078125, "end_time": 1626869166774.285}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 17, "cell_type": "code", "source": "labeledData.take(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[]"}], "metadata": {"cell_status": {"execute_time": {"duration": 2265.60595703125, "end_time": 1626869170404.018}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": 18, "cell_type": "code", "source": "tokenizer = Tokenizer(inputCol=\"violations\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n#hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\nmodel = pipeline.fit(labeledData)\nmodel", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error occurred while calling o226.fit.\n: java.lang.UnsupportedOperationException: empty.min\n\tat scala.collection.TraversableOnce$class.min(TraversableOnce.scala:222)\n\tat scala.collection.mutable.ArrayOps$ofDouble.min(ArrayOps.scala:270)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:523)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\", line 132, in fit\n    return self._fit(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/pipeline.py\", line 109, in _fit\n    model = stage.fit(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\", line 132, in fit\n    return self._fit(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 295, in _fit\n    java_model = self._fit_java(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 292, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o226.fit.\n: java.lang.UnsupportedOperationException: empty.min\n\tat scala.collection.TraversableOnce$class.min(TraversableOnce.scala:222)\n\tat scala.collection.mutable.ArrayOps$ofDouble.min(ArrayOps.scala:270)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:523)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 5296.33203125, "end_time": 1626869180733.279}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "hashingTF.getNumFeatures()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 56.053955078125, "end_time": 1626867705510.155}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "testData = spark.textFile('Food_Inspections2.csv')\\\n             .map(csvParse) \\\n             .map(lambda l: (int(l[0]), l[2], l[3], l[4]))\ntestDf = sqlCtx.createDataFrame(testData, schema).where(\"results = 'Fail' OR results = 'Pass' OR results = 'Pass w/ Conditions'\")\npredictionsDf = model.transform(testDf)\npredictionsDf.registerTempTable('Predictions')\npredictionsDf.columns", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 247.882080078125, "end_time": 1626867762986.256}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "predictionsDf.take(1)", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "numSuccesses = predictionsDf.where(\"\"\"(prediction = 0 AND results = 'Fail') OR \n                                      (prediction = 1 AND (results = 'Pass' OR \n                                                           results = 'Pass w/ Conditions'))\"\"\").count()\nnumInspections = predictionsDf.count()\n\nprint \"There were\", numInspections, \"inspections and there were\", numSuccesses, \"successful predictions\"\nprint \"This is a\", str((float(numSuccesses) / float(numInspections)) * 100) + \"%\", \"success rate\"\n", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.sql.types import *\nfrom IPython.core.magic import register_line_cell_magic", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "# Configuration parameters\nmax_show_lines = 50         # Limit on the number of lines to show with %sql_show and %sql_display\ndetailed_explain = True    ", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "@register_line_cell_magic\ndef sql(line, cell=None):\n    \"Return a Spark DataFrame for lazy evaluation of the sql. Use: %sql or %%sql\"\n    val = cell if cell is not None else line \n    print val\n    return sqlContext.sql(val)\n\n@register_line_cell_magic\ndef sql_show(line, cell=None):\n    \"Execute sql and show the first max_show_lines lines. Use: %sql_show or %%sql_show\"\n    val = cell if cell is not None else line \n    return sqlContext.sql(val).show(max_show_lines) \n\n@register_line_cell_magic\ndef sql_display(line, cell=None):\n    \"\"\"Execute sql and convert results to Pandas DataFrame for pretty display or further processing.\n    Use: %sql_display or %%sql_display\"\"\"\n    val = cell if cell is not None else line \n    return sqlContext.sql(val).limit(max_show_lines).toPandas() \n", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "#%%sql -q -o true_positive\n#SELECT count(*) AS cnt FROM Predictions WHERE prediction = 0 AND results = 'Fail'\ntrue_negative = spark.sql(\"SELECT count(*) AS cnt FROM Predictions WHERE \\\n        (prediction = 0 AND results = 'Fail')\").toPandas()", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "#%%sql -q -o false_positive\nfalse_negative = spark.sql(\"SELECT count(*) AS cnt FROM Predictions \\\nWHERE prediction = 0 AND (results = 'Pass' OR results = 'Pass w/ Conditions')\").toPandas()", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "#%%sql -q -o true_negative\nfalse_positive = spark.sql(\"SELECT count(*) AS cnt FROM Predictions WHERE \\\n       prediction = 1 AND results = 'Fail' \").toPandas()", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "#%%sql -q -o false_negative\ntrue_positive = spark.sql(\"SELECT count(*) AS cnt FROM Predictions WHERE \\\n      prediction = 1 AND (results = 'Pass' OR results = 'Pass w/ Conditions')\").toPandas()", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "false_negative['cnt']", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\n\nlabels = ['True positive', 'False positive', 'True negative', 'False negative']\nsizes = [true_positive['cnt'], false_positive['cnt'], false_negative['cnt'], true_negative['cnt']]\ncolors = ['turquoise', 'seagreen', 'mediumslateblue', 'palegreen', 'coral']\nplt.pie(sizes, labels=labels, autopct='%10.1f%%', colors=colors)\nplt.axis('equal')", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "Precision and recall are then defined as:\n\n$$Precision=\\frac {tp}{tp+fp}$$\n\n\n$$ Recall = \\frac {tp}{tp+fn} $$\n\n\nPrecision is the probability that a (randomly selected) positive prediction is correct.\n\nRecall is the probability that a (randomly selected) resturant with a passing grade is predicted to be passing.\n", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "print('so precision = %f'% \\\n      (float(true_positive['cnt'])/(float(true_positive['cnt'])+float(false_positive['cnt']))))", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "print('and recall = %f'% \\\n      (float(true_positive['cnt'])/(float(true_positive['cnt'])+float(false_negative['cnt']))))", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "If we do this another way, we can ask how accurate are we in finding the failing resturants? This is a bit harder because there are far fewer of them.   In this case we are interested in true-negatives, so \n\nPrecision is the probability that a (randomly selected) negative prediction is correct.\n\nRecall is the probability that a (randomly selected) resturant with a failing grade is predicted to be failing.\n\n\n$$Precision=\\frac {tn}{tn+fn}$$\n\n\n$$ Recall = \\frac {tn}{tn+fp} $$\n", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "print('so the precision of failure prediction = %f'% \\\n      (float(true_negative['cnt'])/(float(true_negative['cnt'])+float(false_negative['cnt']))))", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "print('and recall is = %f'% \\\n      (float(true_negative['cnt'])/(float(true_negative['cnt'])+float(false_positive['cnt']))))", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}, "anaconda-cloud": {}}}